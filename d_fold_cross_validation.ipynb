{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D-fold cross technique\n",
    "\n",
    "\n",
    "it can be used for two different purposes:  \n",
    "1. **validation**  \n",
    "2. **tuning**\n",
    "\n",
    "\n",
    "\n",
    "### validation:\n",
    "- sometimes we don't want to waste any of our data with spiting it into test and train (perhaps a small dataset?)\n",
    "- or maybe we want to make sure our testing set is as difficult as our train set\n",
    "in these scenarios we can use this technique; this is how you can do that:\n",
    "1. split your dataset into d subsets\n",
    "2. train on all of them except one\n",
    "3. do it again but this time use another piece as test\n",
    "4. repeat the process d times\n",
    "5. calculate the error for each of them and then calculate the average error\n",
    "\n",
    "not only have you access to the **average error** with this method, but you also have a **range which estimate the error** in the future datasets\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### tuning\n",
    "\n",
    "the very same technique can be used for tunning:\n",
    "\n",
    "1. we can try to **minimise the cost function** using it\n",
    "2. or we can minimize the error range \n",
    "which means we can use it for tunning the errors, in other words, minimising the maximum error instead of the average error.\n",
    "3. the confidence interval of the classification error of this method is more realistic compared to bootstrapping and is usually bigger (as bootstrapping may underestimate confidence interval due to its repetitions)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "- > this technique is also known as k-fold instead of d-fold\n",
    "- > obviously, it is not an efficient technique"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
