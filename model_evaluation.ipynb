{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "\n",
    "depending on if we have a classification or regression model, we have different types and methods for evaluations.  \n",
    "\n",
    "\n",
    "\n",
    "## regressions:\n",
    "- goal is to build a model to accurately predict an unknown case\n",
    "- we usually split our data into train and test so we can prevent **over-fitting**\n",
    "- a cost function measures how well the hypothesis h(x,w) fits the training data.\n",
    "\n",
    "methods to calculate error:\n",
    "- <strong> MAE</strong>:\n",
    "    - <strong> Mean Absolute Error</strong>\n",
    "    - $ \\frac{1}{n} \\sum_{i=n}^{n} |y_{i} - \\hat y_{i}| $\n",
    "    - its just calculate the absolute value so negative and positive errors don't cancel each other.\n",
    "    - Gives equal weight to all errors.\n",
    "    - Robust to outliers because it doesn't heavily penalize large errors.\n",
    "\n",
    "    <br/>\n",
    "\n",
    "- <strong> MSE</strong>:\n",
    "    - <strong> Mean Squared Error</strong> :\n",
    "    - $ \\frac{1}{n} \\sum_{i=n}^{n} (y_i - \\hat{y}_i)^2 $  \n",
    "    - Measures the average squared differences between actual and predicted values.\n",
    "    - Squaring the errors gives more weight to larger errors, making it sensitive to outliers.\n",
    "    - Often used in optimization problems because of its differentiability.\n",
    "    <br/>\n",
    "\n",
    "- <strong> RMSE </strong>\n",
    "    - <strong> Root Mean Squared Error</strong>\n",
    "    - $ \\sqrt{MSE} $\n",
    "    - Derived from MSE, it represents the square root of the average squared differences between actual and predicted values.\n",
    "    - Like MSE, it gives more weight to larger errors but is in the original unit of the data.\n",
    "    - Commonly used when a metric in the original data unit is preferred.\n",
    "\n",
    "    <br/>\n",
    "- <strong> RAE </strong>\n",
    "    - <strong> Relative Absolute Error</strong>\n",
    "    - $ \\frac {\\frac{1}{n} \\sum_{i=n}^{n} |y_{i} - \\hat y_{i}|} {\\frac{1}{n} \\sum_{i=n}^{n} |y_{i} - \\bar y_{i}|} $\n",
    "    - Measures the proportion of absolute prediction error relative to the absolute error of the mean model.\n",
    "    - Provides a ratio of how well the model performs compared to a naive mean model.\n",
    "    - A lower RAE indicates a better model.\n",
    "\n",
    "    <br/>\n",
    "- <strong> RSE </strong>\n",
    "    - <strong> Relative Squared Error</strong>\n",
    "    - $ \\frac {\\frac{1}{n} \\sum_{i=n}^{n} (y_{i} - \\hat y_{i})^2} {\\frac{1}{n} \\sum_{i=n}^{n} (y_{i} - \\bar y_{i})^2} $\n",
    "    - Similar to RAE but uses squared differences.\n",
    "    - Measures the proportion of squared prediction error relative to the squared error of the mean model.\n",
    "    - A lower RSE indicates a better model.\n",
    "- <strong> R2 (Coefficient of Determination) </strong> \n",
    "    - $ R^2 = 1 - RSE   $\n",
    "    - Represents the proportion of the variance in the dependent variable that is predictable from the independent variables.\n",
    "    - Ranges from 0 to 1, where 1 indicates a perfect fit.\n",
    "\n",
    "\n",
    "\n",
    "--- \n",
    "\n",
    "**tips**:\n",
    "- obviously you we only square the errors when they are larger than 1, otherwise we make them smaller\n",
    "- numpy has methods for mean and absolute mean but for r2_score you can use the sklearn.metrics.r2_score\n",
    "\n",
    "\n",
    "\n",
    "## classification\n",
    "\n",
    "- accuracy isz not always a good metric\n",
    "    - face detection\n",
    "    - accuracy of a classifier that always says 'no' is 99.9999%\n",
    "here you have 3 main methods for evaluating:\n",
    "1. F-1 score\n",
    "    - precision and recall:\n",
    "        - True positive: selected elements that are relevant\n",
    "        - False positive: selected elements that are irrelevant\n",
    "        - True negative: missed elements that are irrelevant\n",
    "        - False negative: missed elements that are relevant\n",
    "\n",
    "    <small>\n",
    "        sometimes one of them are more important from another for example maybe we don't care if we do some preventions for something that will not happened(FP),\n",
    "        but we don't want to something unexpected happened (FN)\n",
    "    </small>\n",
    "\n",
    "    - Precision = TP / (TP + FP)  \n",
    "    - Recall = TP / (TP + FN)  \n",
    "    - F1-score = $ 2  \\frac {Precision + Recall}{Precision + Recall} $\n",
    "    - this kind of mean is called **harmonic mean**\n",
    "2. jaccard index:  \n",
    "    $ y $ : Actual Labels  \n",
    "    $ \\hat{y} $ : Predicted Labels  \n",
    "    $ J (y , \\hat{y}) = \\frac{| y \\cap  \\hat{y}| }{ | y \\cup \\hat{y} | } = \\frac { | y \\cap  \\hat{y} | } {|y| + |\\hat{y}| - | y \\cap  \\hat{y}|}  $\n",
    "\n",
    "    $ y $ : [0, 0, 0, 1, 1, 0, 1, 1, 0, 1]  \n",
    "    $ \\hat{y} $ : [1, 1, 0, 1, 1, 0, 1, 1, 0, 1]  \n",
    "    \n",
    "    there is a total of 10 labels where we predict 8 of them correctly:  \n",
    "    $ j(y, \\hat{y}) = \\frac {8} {10 + 10 - 8}  = 0.66 $  \n",
    "\n",
    "    sklearn.metrics.jaccard_score, will calculate the value for each label and it can return the average:\n",
    "    the average method can be change as this:\n",
    "    - None, the scores for each class are returned.\n",
    "\n",
    "    - 'binary':\n",
    "    Only report results for the class specified by pos_label. This is applicable only if targets (y_{true,pred}) are binary.\n",
    "\n",
    "    - 'micro':\n",
    "    Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "\n",
    "    - 'macro':\n",
    "    Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
    "\n",
    "    - 'weighted':\n",
    "    Calculate metrics for each label, and find their average, weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance.\n",
    "\n",
    "    - 'samples':\n",
    "    Calculate metrics for each instance, and find their average (only meaningful for multilabel classification).\n",
    "\n",
    "3. log loss:\n",
    "\n",
    "    since we are getting a categorical value for the labels,\n",
    "    they may not be accurate for example:\n",
    "    imagine we put person1, person2 in group 1, but we were so certain about person one but not so certain about person2 \n",
    "    we can calculate this error with log-loss\n",
    "\n",
    "    LogLoss = $ - \\frac{1}{n} \\sum(y \\times \\log(\\hat{y}) + (1 - y) \\times log(1 - \\hat{y})) $  \n",
    "    \n",
    "    $ 0 \\le $ LogLoss $ \\le 1  $\n",
    "\n",
    "    less Log-loss means more accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
